---
output:
  pdf_document: default
  html_document: default
---
# Moving towards large-scale data-intensive forecasting of species richness in breeding birds
# Using best practices for forecasting biodiversity changes in breeding birds
Authors: Harris, Taylor and White?

`r settings = yaml::yaml.load_file("settings.yaml")`

## Introduction

Forecasting the future state of ecological systems is increasingly important for
planning, management, and evaluating how well ecological models capture the key
ecological processes governing a system [@clark2001, @dietz2017]. One area of
particular interest for forecasting is biodiversity since it is important for
ecosystem function, central to conservation planning, and expected to be
influenced by anthropogenic global change. Anticipating potential changes in
biodiversity is crucial for large scale management and conservation, and for
addressing debates regarding whether local scale diversity is declining and may
continue to decline in response to anthropogenic influences [@dornelas2014].

Previous efforts to predict future patterns of species richness, and diversity
more generally, have focused primarily on building species distributions models
to quantify the spatial relationships between the occurrence of individual
species and the environment. Forecasts for future environmental conditions are
then used to predict where each species will occur in the future and the
probabilities of occurrence are summed across species to predict future species
richness [e.g., @thomas2004]. Alternatively, models that directly relate spatial
patterns of species richness to the environment have been developed and
generally perform equivalently to species distribution modeling based methods
[@algar2009, @distler2015].

However, despite the emerging interest in forecasting species richness and other
aspects of biodiversity, little is known about how effectively we can anticipate
their dynamics. This is due in part to the long time scales over which many
ecological forecasts are applied and the resulting difficulty in assessing
whether the predicted changes occurred. What we do know comes from a small
number of hindcasting studies, where models are built using data on species
occurrence and richness from the past and evaluated on their ability to predict
contemporary patterns [e.g., @algar2009, @distler2015]. These studies are a
valuable first step, but lack several components that are important for
developing good forecasting models and understanding how accurately these models
can predict the future. These components of good forecasting and evaluation (Box
1) broadly involve: 1) expanding the use of data to include biological and
environmental time-series [@treddenick2016]; 2) accounting for uncertainty in
observations, processes, model choice, and forecast evaluation
[@clark2001, @dietz2017]; and 3) conducting meaningful evaluations of the
forecasts by hindcasting, archiving short-term forecasts, and comparing
forecasts to baselines to determine if the forecasts are more accurate than
assuming the system is basically static [@Perretti2013].

In this paper we attempt to forecast the species richness of breeding birds at
hundreds of locations throughout North America while following the best
practices in Box 1 for ecological forecasting. To do this we combine 30 years of
time-series data on bird distributions with monthly time-series of climate data
and satellite-based remote-sensing. This 30 year time-scale has only recently
become possible for large-scale time-series based forecasting and allows us to
model and assess changes a decade or more into the future in the presence of
shifts in environmental conditions on par with predicted climate change. We
compare traditional distribution modeling based approaches to spatial models of
species richness, time-series methods, and simple baselines. All forecasts model
uncertainty and observation, are evaluated across different time lags using
hindcasting, and are publicly archived to allow future assessment. We discuss
the implications of these practices for our understanding of, and confidence in,
the resulting forecasts, and how we can continue to improve on these approaches
in the future.


## Methods

### Data

* BBS + retriever
* Predictor variables
  * Monthly climate data from interpolated weather stations (PRISM), converted
    to BIOCLIM variables. Many of these variables were tightly correlated, so
    we only included the 8 variables selected by Harris's (2015) BBS analysis
  * Weather/climate and NDVI were each calculated separately for each year,
    rather than averaged over the full multi-decade period (as Worldclim does)
  * NDVI (GIMMS)
  * Altitude (SRTM 90m Digital Elevation Database via raster::getData)
* Data summary: 
  * Number of sites, species, unique observers
  * central moments of richness (mean about 51, sd about 12 spp)
  * The within-site variation was much smaller (sd about 5 spp), making the
    site-level distributions underdispersed.  
    * This underdispersion implies
      that common count models such as binomial, Poisson, negative binomial 
      won't fit well.
    * For this reason (and because the time series methods assumed 
      Gaussian errors), we defined model performance throughout this paper in 
      terms of Gaussians (e.g. mean squared error), rather than with count
      distributions.
        * Some of the modeling approaches (SDMs and JSDMs) did use discrete
          distributions internally, however.
* Splitting the data temporally
  * Our data set began in `r settings$start_yr`, which was the first year that 
    NDVI data was available from GIMMS
  * All the models were trained on the data that was collected through 
    `r settings$last_train_year`
  * Sites were included in the analysis if there were at least 
    `r settings$min_num_yrs` years of data available during this period.
  * The models were tested/evaluated on the data from 
    `r settings$last_train_year + 1` `r settings$end_yr`.

### Accounting for site \& observer effects
* Observer effects are inherent in large data sets taken by different 
  observers, and are known to occur in BBS [citation].
* While these biases are known to exist, they are difficult to quantify: even if
  the different observers ran the same transects under identical conditions (which 
  they don't), we would still only have an estimate of their differences, not an 
  exact value.
* We estimated the observer effects, along with our uncertainty about those
  effects' size & direction, with a linear mixed model.
* This model partitions the variance in observed richness values into site-level
  variance, observer-level variance, and residual variance (e.g. variation within
  a site from year to year or from morning to morning).
  * No fixed effects in this model (e.g. environmental predictor variables because 
    we're not interested in explaining differences among sites at this point; we 
    just want to account for their existence while estimating observer effects.
  * Subtracting off the estimated observer effect provides an estimate of how many
    would have been found by a "typical" observer on that day; in this sense, it 
    enables us to correct for observer differences. We then used this "corrected" 
    version of the data for our time-series models such as the AR1, etc.
* A simple model like this one can't tell us exactly how each observer differed from
  the others, so it is important to represent our uncertainty about these differences.
  Here, we did so by collecting [[N]] Monte Carlo samples from the model's posterior
  distribution, using Stan. This provided us with [[N]] plausible values for each 
  observer, so we did not have to commit to a single value.
* While some methods for predicting richness might be able to use the uncertain
  estimates of the observer effects directly, we take the simpler approach of just
  fitting each of the downstream models with each of the Monte Carlo samples. This
  increases the amount of computational work by a factor of [[N]], but CPU time
  wasn't our limiting resource.

### Site-level models

[[note that these all use a Gaussian error distribution]]

**Baseline models.** The two simplest models we fit treated site-level richness
observations either as uncorrelated noise around a site-level constant (the 
"average" model) or as an autoregressive model with a single year of history 
(the "naive" model). The predictions from these two models are qualitatively 
different: predictions from the "average" model are centered on the average 
richness and the confidence intervals are narrow and constant-width. The
"naive" model, in contrast, predicts that future observations will be similar to
the final observed value (e.g. the value observed in `$r settings$last_train_year`), 
and the confidence intervals expand rapidly as the predictions extend farther into 
the future.  Each of these model types was fit separately to each BBS route in two
ways: first, we fit the model to the raw richness values, without accounting for
possible differences among observers at the site.  Then, we fit the models to the
residuals from our observer model (i.e. fitting the model to an estimate of how
many species a typical observer would have found, rather than to the raw data). 
In the latter case, we averaged across [[N]] models that were fit separately to 
the [[N]] Monte Carlo estimates of the observer effects, to account for our
uncertainty in the true values of those effects.

**Time series models.** Like the two baseline models, these models only looked at 
one site at a time, but they could include additional complexity. As above, these
models were trained both with the raw richness time series and with Monte Carlo
estimates of the observer-corrected time series. The  Auto-ARIMA models (based on 
Hyndman's 2016 `auto.arima` function) could include an autoregressive component 
(as in the "naive" model, but with the possibility of longer-term dependencies
in the underlying process), a moving average component (where the noise can have 
serial autocorrelation) and an integration/differencing component (so that the 
analysis could be performed on sequential differences of the raw data, rather than
on the observed time series itself). The `auto.arima` function chooses whether to 
include each of these components, and how many terms to include for each one, using
AICc, as described in Hyndman (2016). We used the default settings for this function,
apart from setting `seasonal = FALSE` because our time series does not include a 
seasonal component.  [[Note: should probably record somewhere what order model
the auto.arima function tends to choose.  Eyeballing the results, seems to be most 
similar to the naive model, but would be good to check.]] We also fit an 
"Auto-ARIMA + environment" model, in which each site-level time series could 
also be predicted by annual changes in our environmental variables at that site.

### Continental-scale models

So far, each model has been fit to one site at a time, with no information 
(apart from observer effects) shared among sites. When continental-scale data 
is available, ecologists often use correlative models to predict richness 
changes over time from environmental data. Here, we tested three variants of 
this approach. Here, we did *not* include the site-level random effects as 
predictors, meaning that this approach implicitly assumes that two sites 
with identical Bioclim, elevation, and NDVI values should have identical 
richness distributions. As above, we included observer effects & the 
associated uncertainty by running this model [[N]] times (once per MCMC 
sample).

**"Macroecological" model.** The most straightforward way to predict richness is 
to fit a richness model. We chose the `gbm` package for fitting boosted 
regression trees as our example of such a model. Boosted regression trees is a 
common approach in the species distribution modeling literature [refs] and 
works by fitting thousands of small tree-structured models sequentially, with
each tree optimized to reduce the error of its predecessors. For the reasons 
discussed above, this model was optimized using a Gaussian likelihood. [[other 
gbm settings go here.]] Compared to SDMs, this lets us predict richness directly 
instead of using hundreds of noisy species-level estimates, but it also discards 
any potentially-useful information regarding species turnover.

**SDMs.**
* Probably the most common approach?
* These models say that species' occurrence probabilities depend only on the values of
  the environmental variables at the time and place of the transect, not on the history
  of the environment or the history of the birds at that site
* Predict each species' occurrence probabilities individually, then add up the predictions
* [[Something about not using thresholds that throw away information]]
* Taking this approach in the face of uncertainty about observer effects would either 
  require a customized model (cite "neighborly advice" paper), or fitting [[N]] models
  times the number of species.
* To make it feasible to fit this large number of models, we used a custom wrapper 
  around the randomForest package. Random forests are flexible models that are built 
  by combining many independent sub-models ("trees") fit to different versions of the training
  data, which made it straightforward to adapt them to our situation: we simply fit one 
  tree using each of the Monte Carlo estimates of our observer effects and averaged the 
  predictions of the resulting forest.
* For the uncertainty, we used Gaussian approximations, based on the means and variances
  of the sums of independent Bernoulli random variables (cite Calabrese et al.).

**JSDM** 
* Joint species distribution model (JSDM) is a new approach that makes predictions
  about the full community composition, rather than modeling it species-by-species.
* In stacked single-species SDMs, the species-level occurrences are treated as 
  independent coin-flips whose proability depends only on the environment.
* JSDMs remove this independence assumption, and explicitly account for the
  possibility that a site will be much more (or less) suitable for birds in 
  general than one would expect based on a few environmental measurements alone.
* As a result, JSDMs do a better job of representing our uncertainty about 
  richness, while stacked SDMs underestimate it (Harris 2015).
* We used the `mistnet` package (Harris 2015) because it is the only JSDM that
  describes species' environmental associations with nonlinear functions.
* The mistnet package doesn't "know" about time series or spatial autocorrelation,
  so it did not share information among the repeated runs of the same transect.

### Ensembles
* Ensembles. Averaging predictions from multiple models tends to reduce the 
  noise in the estimates, and can thus lead to better predictions. Choosing the
  weights for the ensemble isn't straightforward because our estimates of model 
  error on the training set are biased in favor of models that
  overfit. [[Another option is to spatially cross-validate on the training set]]

### Model evaluation

* Evaluated each year between  `r settings$last_train_year` and `r settings$end_yr`
* When a model was run multiple times (e.g. once per MCMC sample for the observer
  estimates), we used the mean of those runs' point estimates as our final point 
  estimate and we calculated the uncertainty using the law of total variance (i.e.
  the average of the model runs' variance, plus the variance in the point estimates).
* Metrics:
  * (R)MSE 
    * How far, on average, are the models' predictions from the true value?
    * [[RMSE has better units than MSE (species versus squared species), but MSE is
      nice because it's additive (i.e. MSE of 2 is twice as big as MSE of 1).]]
  * Calibration/bias
    * In general, or in specific situations, is the model consistently predicting
      that richness will be too high or too low?
    * Can also report this value in terms of RMSE or MSE
  * Coverage of 95% confidence intervals
  * Gaussian deviance (simultaneously rewards good point estimates, precision, 
    and coverage)

Looked at how error changed as the time horizon of forecasting lengthened
* Error will generally tend to increase
* Error might flatten out after some number of years
* Error might increase at different rates for different methods
  * One model might be better at short time scales \& worse at long time scales

## Results and conclusions

* The mixed model (which formed the basis of our "average" predictions) found 
  that 70% of the variance in richness in the training set could be explained 
  by differences among sites, and 21% could be explained by differences among 
  observers. The remaining 9% represents residual variation, where a given 
  observer might report a different number of species on different runs of the 
  same transect.
* Over the time period we analyzed, site-level richness was mostly stationary 
  once observer effecs were accounted for, which explains the success of the 
  "average" baseline.
* If site-level richness was approximately stationary, then why does error 
  increase over time? One would expect constant error under stationarity.  
  This apparent contradiction can be resolved by examining [[Figure]]: most of 
  the increase in error was due to an influx of observers whose tendencies were
  poorly identified. Looking across the rows of panel A, we see very little 
  change in MSE over time for a given type of observer, but panel B shows a 
  steady increase in the number of observers whose estimates could only be 
  identified to within +/- 5 species (or worse) because they did not make
  enough observations in different sites in the training set to determine
  their [[biases]] any more precisely.

* None of the models reliably outperformed both baselines. Depending on the 
  criterion and whether observer differences were accounted for, the best 
  model was usually "naive" or "average." The "auto.arima" and "richness-gbm" 
  models occasionally produced less error than either baseline, but their MSE 
  values still averaged substantially higher than the baselines. Of the two 
  baselines, predictions based on the long-term mean ("average") were more 
  reliable if observer effects could be accounted for, while predictions based 
  on the most-recently observed value ("naive") were better prepared to deal 
  with observer turnover. In general, the models produced confidence intervals 
  that were too narrow; [[discuss naive and possibly mistnet here]].
* The models that were fit to individual sites (i.e. the baselines and the 
  ARIMA methods) got most or all of the inter-site variation by default, 
  which made them difficult to beat. The SDMs and GBM-richness could only 
  outperform the baselines on intra-site variation, which represented a very 
  small portion of the variance; in general, the inter-site errors were large 
  enough to overpower any such gains.
* Consistent with Distler (2015, Journal of Biogeography), we found that the 
  stacked SDMs (based on random forests) and the nonlinear regression of 
  richness on environment ("gbm") produced point estimates of similar quality, 
  though SDMs were somewhat less accurate in our case. However, we found large 
  differences between the two models in their ability to accurately report 
  uncertainty; the stacked SDMs' confidence intervals were only half of their 
  nominal widths, on average, producing the worst coverage and highest deviance 
  of any method we tested.
* [[Mistnet goes here; point estimates were worse than RF but coverage was very good]]
* Temporal richness variability doesn't track the predictors we use; sites have "inertia"
  * Cite 3 papers Ethan found on consistancy of alpha even when composition changes dramatically
  * Good for absolute accuracy, little room for improvement over baseline
  * Maybe richness is too coarse
    * abundance
    * species-level
* More important to use a reasonable error model than to get the best predictors \& basis expansions
* Ensembles help/don't help (depending on results)
* Point estimate error versus coverage
* In many ways, this is the best-case scenario for SDMs:
  * large data set, known \& consistent sampling technique over a period of decades
  * "True" absences
  * extended predictor set
  * birds arguably migrate quickly enough to make equlibrium \& especially space-for-time assumptions less ridiculous
  * errors for individual species can cancel out; only asking for aggregate values
  * Counterpoint: 
    * if most variation is among sites, maybe we want site-level means for predictors rather 
      than annual values (or, why not use both?)
* Value of stacking versus predicting the quantitity of interest directly
* Need to start doing this, even if we're bad at it.
  * We've posted predictions for the next N years of BBS data, using several methods, 
    and invite others to do so as well.
* Dig around in the residuals for insights
* SDM's and time series models may not be accounting for the ecology of north 
  american birds. Most of them migrate, so whether they are observed or not at 
  the same time every year is a function of their migratory phenology. The 
  migratory patterns of several species have been correlated with NDVI (Renfrew 
  et al 2013). 


## misc

SDM assumptions:
  * we've measured the predictive power of all avialable variables, other potentially important thing that aren't available are landcover, observer skill level, (probably some others)
  * Something like stationarity:
    * e.g. "systems are at equilibrium now and will be at equilibrium in future"
    * or "species track predictor variables quickly" (no lags)
    * etc
  * independent/identically distributed observations across species, years
  * Space substitutes well for time

None of the models are true forcasts as they use observed predictor variables (except the AR1). 

* Need predictions at multiple time scales
* Prediction quality at short time scales might not be a good indicator of 
  long-term effectiveness

## Box 1: Ten simple rules for making and evaluating ecological forecasts

### 1. Compare multiple modeling approaches

Typically ecological forecasts use one modeling approach or a small number of
related approaches. By fitting and evaluating multiple modeling approaches we
can learn more rapidly about the best approaches for making predictions for a
given ecological quantity. This includes comparing process based and data-driven
models and comparing the accuracy of forecasts to simple baselines to determine
if the modeled forecasts are more accurate than the naive assumption that the
world is static.

### 2. Use time-series data when possible

Forecasts describe how systems are expected to change through time. While some
areas of ecological forecasting focus primarily on time-series data, others
primarily focus on using spatial models and space-for time substitutions. Using
ecological and environmental time-series data allows the consideration of actual
dynamics from both a process [@treddenick2016a] and error structure perspective.

### 3. Pay attention to uncertainty

Understanding confidence in a forecast is just as important as understanding the
average or expected outcome. Failing to account for uncertainty can result in
overconfidence in highly uncertain outcomes leading to poor decision making and
erosion of confidence in ecological forecasts. Models should explicitly include
sources of uncertainty and how the propagate through the forecast where
possible. Evaluations of forecasts should assess the accuracy of uncertainties
as well as point estimates [@dietz2017].

### 4. Use predictors related to the question

Many ecological forecasts use data that is readily available and easy to work
with. While ease of use is a reasonable consideration it is also important to
include predictor variables that are expected to relate to the ecological
quantity being forecast and dynamic time-series of predictors instead of
long-term averages. Investing time in identifying and acquiring better predictor
variables may have at least as many benefits as using more sophisticated
modeling techniques.

### 5. Assess how forecast accuracy changes with time-lag

In general the accuracy of forecasts decreases with the length of time into the
future being forecast [@petch2015]. This decay in accuracy and the potential for
different rates of decay to result in different relative model performance at
different lead times should be considered when evaluating forecasts and
comparing models.

### 6. Include an observation model

Ecological observations are influenced by both the underlying processes and how
the system is sampled. When possible forecasts should model the factors
influencing the observation of the data.

### 7. Validate using hindcasting

To evaluate the expected accuracy and uncertainty of forecasts assess the
performance of these forecasts within existing time-series data.

### 8. Publicly archive forecasts

To allow the future evaluation of the accuracy and uncertainty of forecasts the
forecast values and/or models should be archived so that they can be assessed
after new data is generated [@mcgill2012]. Enough information should be provided
to allow an unambiguous assessment of the forecast performance.

### 9. Make short-term and long-term predictions

In cases where long-term predictions are the primary goal, short-term should
also be made to accommodate the time-scales of planning and management decisions
and to allow the accuracy of the forecasts to be quickly evaluated
[@treddenick2016a].

### 10. [[?]]

Sites' biotic and abiotic environments differ in more ways than the 
few ones you happened to measure. As a result, some sites will 
consistently have more species than a regression model (or sum of 
regression models) would predict, while others willl consistently have 
fewer. Depending on the spatial scale of these unmeasured variables, 
independent site-level random effects could be sufficient for dealing 
with this, or spatially-autocorrelated random effects might be needed.